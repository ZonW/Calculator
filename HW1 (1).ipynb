{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## HW 1: Math Foundation and Python Programming","metadata":{}},{"cell_type":"markdown","source":" <div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>","metadata":{}},{"cell_type":"markdown","source":"### Objectives: ","metadata":{}},{"cell_type":"markdown","source":"The objectives of this assignment include:\n- Test your Python programming skills\n- Program deep learning models from scrath\n- Understand gradients, backpropagtion, and the impact of activation/loss functions on model performance\n- Learn to analyze model performance analytically ","metadata":{}},{"cell_type":"markdown","source":"### Problem Description","metadata":{}},{"cell_type":"markdown","source":"- Suppose you have a two-layer deep learning model as shown below: \n    - $ L = g~(z^{(2)}, y)$, where $y$ is ground-truth, and $g$ is a loss function. \n    - $ z^{(2)} = \\sigma~(~u^{(2)}~), ~ u^{(2)} = A^{(2)~T} z^{(1)} + b^{(2)},\\text{where}  ~z^{(2)}, b^{(2)} \\in R,~ z^{(1)},A^{(2)} \\in R^3, ~\\text{and} ~\\sigma $ is the `sigmoid` activation function.\n    - $ z^{(1)} = f ~(~u^{(1)}~),~ u^{(1)} = A^{(1)} x + b^{(1)}, \\text{where}~x \\in R^2, ~z^{(1)},~b^{(1)} \\in R^3, A^{(1)}\\in R^{3 \\times 2}$, and $f$ is an activation function.\n    \n    \n- Now, you have two options for $f$ and $g$:\n    - $f$: `ReLU` or `tanh`\n    - $g$: `cross_entropy` $L=-\\frac{1}{N}\\sum_i[~y_i*\\ln{z^{(2)}_i}+(1-y_i)*ln~{(1-z^{(2)}_i)}~]$, or `mean squared error (mse)`, $L=\\frac{1}{2N}\\sum_i(y_i- z^{(2)}_i)^2$, where $i\\in [1, 2, ..., N]$ are training instances.\n    ","metadata":{}},{"cell_type":"markdown","source":"<img src='hw1.png' width = '50%'>","metadata":{}},{"cell_type":"markdown","source":"Following the instruction below to program your solution in Python notebook step by step carefully.\n\n\nIn this assignment, `you don't need PyTorch. You only need to use Numpy Package`. The following are useful numpy functions: `np.dot`, `np.exp`, `np.tanh`, `np.squeeze`, `np.eye`, `np.zeros`, `np.expand_dims`, `np.fill_diagonal`. Do your search to figure out how to use these functions.\n\n\nFor the sake of convenience, in the code, we use `A1, A2, b1, b2, u1, u2, z1, z2` to denote $A^{(1)}, A^{(2)}, b^{(1)}, b^{(2)}, z^{(1)}, z^{(2)}$ respectively.","metadata":{}},{"cell_type":"markdown","source":"### Requirements","metadata":{}},{"cell_type":"markdown","source":"1. (2 points) Write a function to calculate each of the following partial derivatives. The inputs to the function are all the variables in the model and the returned derivatives are expressions of these variables. Pay attention to the shape of each gradient. An example is given below.\n\n- $ \\frac{d{L}}{du^{(2)}}$, a scalar\n- $ \\frac{\\partial{u^{(2)}}}{\\partial{z^{(1)}}} \\in R^{1\\times 3}$ \n- $ \\frac{\\partial{u^{(2)}}}{\\partial{A^{(2)}}} \\in R^{1\\times 3}$\n- $ \\frac{\\partial{u^{(2)}}}{\\partial{b^{(2)}}}$, a sclar\n- $ \\frac{\\partial{z^{(1)}}}{\\partial{u^{(1)}}} \\in R^{3\\times 3}$\n- $ \\frac{\\partial{u^{(1)}}}{\\partial{A^{(1)}}} \\in R^{3\\times 3 \\times 2}$\n- $ \\frac{\\partial{u^{(1)}}}{\\partial{b^{(1)}}} \\in R^{3\\times 3}$\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom matplotlib import pyplot as plt","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def du1_dA1(u1, x, A1):\n    \n    # make Jacobian tensor with all 0s\n    g = np.zeros(u1.shape + A1.shape)\n    \n    # set the gradient values\n    for i, _ in enumerate(u1):\n        g[i,i] = x\n        \n    return g\n\n\ndef dL_du2(L, y, u2, z2, fun = 'cross_entropy'):\n    # dL/du2 = dL/dz2 * dz2/du2\n    if fun == 'cross_entropy':\n        return z2 - y\n    elif fun == 'mse':\n        return (-2*y+2*z2)*(z2-z2*z2)\n    \ndef du2_dz1(A2):\n    return np.transpose(A2)\n\ndef du2_dA2(u2, z1, A2):\n    return z1\n\ndef du2_db2(u2, b2):\n    mtx = np.zeros((1, 1))\n    np.fill_diagonal(mtx,1)\n    return mtx\n\ndef dz1_du1(z1, u1, fun = 'relu'):\n    return (u1 >= 0)*1\n    \n    \n\ndef du1_db1(u1, b1):\n    mtx = np.zeros((3, 3))\n    np.fill_diagonal(mtx,1)\n    return mtx\n\n    ","metadata":{"tags":[],"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[[1.]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"2. (1 point) Write a function `forward(A1, A2, b1, b2, x, y, act_fun = 'relu', loss_fun = 'cross_entropy')`, which calculates `u1, z1, u2, z2`, and `loss` based on the provided model inputs and parameter values.","metadata":{}},{"cell_type":"code","source":"def forward(A1, A2, b1, b2, x, y, act_fun = 'relu', loss_fun = 'cross_entropy'):\n    \n    u1, z1, u2, z2, loss  = None, None, None, None, None\n    \n                   \n    return u1, z1, u2, z2, loss              ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. (2 point) Write a function `backpropagate(L, x, y, u1, u2, z1, z2, A1, A2, b1, b2, act_fun = 'relu', loss_fun = 'cross_entropy')` to compute the gradients $\\frac{\\partial{L}}{\\partial{A^{(1)}}}, \\frac{\\partial{L}}{\\partial{A^{(2)}}}, \\frac{\\partial{L}}{\\partial{b^{(1)}}}, \\frac{\\partial{L}}{\\partial{b^{(2)}}}$ using the functions defined in Q1. The inputs to this function are all the variables defined in the model and the chosen $f, g$ function names. This function return these 4 gradients.","metadata":{}},{"cell_type":"code","source":"def backpropagate(L, x, y, u1, u2, z1, z2, \\\n                  A1, A2, b1, b2, \\\n                  act_fun = 'relu', \\\n                  loss_fun = 'cross_entropy'):\n    \n    g_A1, g_b1, g_A2, g_b2 = None, None, None, None\n    \n    # add your code\n    \n    return g_A1, g_b1, g_A2, g_b2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. (0.5 point) Write a function `gradient_desc(v, g, lam)` to adjust a parameter value $v$ by its gradient $g$, i.e. return the new value of parameter $v = v - lam*g$, where `lam` is a given learning rate.\n\n\nNote, The shape of your gradient may be different from that of $v$. Check the shapes of $v, g$ to make sure they are the same.","metadata":{}},{"cell_type":"code","source":"def gradient_desc(v, g, lam):\n    \n    # add your code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. (2 points) Write a function `train(x, y, A1, A2, b1, b2, lam = 0.001, act_fun = 'relu', loss_fun = 'cross_entropy', epoch = 100)`: as follows:\n   1. Input arguments are:\n        - Training sample $(x, y)$. For simplicity, let's assume there is *only one* training instance.\n        - Initial parameter values $A1, A2, b1, b2$.\n        - Learning rate `lam`, epochs, and the chosen function names for $f, g$\n       \n   1. Use a loop of `epoch` rounds to do the following:\n       - Call the `forward` function to run a foward pass \n       - Call the `backpropagate` function to compute gradients $\\frac{\\partial{L}}{\\partial{A^{(1)}}}, \\frac{\\partial{L}}{\\partial{A^{(2)}}}, \\frac{\\partial{L}}{\\partial{b^{(1)}}}, \\frac{\\partial{L}}{\\partial{b^{(2)}}}$ \n       - Call the `gradient_desc` function to update parameters $A^{(1)}, A^{(2)}, b^{(1)}, b^{(2)}$\n       \n   1. Return of the loss and $z^{(2)}$ obtained in each epoch, and the final values of and the values of $A^{(1)}, A^{(2)}, b^{(1)}, b^{(2)}$.","metadata":{}},{"cell_type":"code","source":"def train(x, y, A1, A2, b1, b2, \\\n          lam = 0.001, act_fun = 'relu', \\\n          loss_fun = 'cross_entropy', \\\n          epoch = 100):\n    \n    loss_hist = []  # loss curve\n    pred_hist = []  # prediction curve\n\n    for i in range(epoch):\n    \n    # add your code\n              \n    return A1, b1, A2, b2, loss_hist, pred_hist","metadata":{},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"6. (0.5 point) Test your your `forward` and `backpropagate` functions as follows:\n    - Initial parameter values and the training sample are set as follows: \n        - `A1 = [[-2,1],[2, -2],[-1, 5]], b1 = [3, -1, 2]`\n        - `A2 = [1, -2, 2], b2 = 0`\n        - `x = [0.465, 0.178], y = 0`\n   \n    - Call your `forward` and `backpropagate` functions under two cases:\n        1. `act_fun = 'relu', loss_fun = 'cross_entropy'`\n        1. `act_fun = 'tanh', loss_fun = 'mse'`\n        \n    - Carefully check the obtained `u1, z1, u2, z2, L` and the values of gradients $\\frac{\\partial{L}}{\\partial{A^{(1)}}}, \\frac{\\partial{L}}{\\partial{A^{(2)}}}, \\frac{\\partial{L}}{\\partial{b^{(1)}}}, \\frac{\\partial{L}}{\\partial{b^{(2)}}}$ to make sure they are correct.","metadata":{}},{"cell_type":"code","source":"A1 = np.array([[-2,1],[2, -2],[-1, 5]])\nA2 = np.array([1, -2, 2])\nb1 = np.array([3, -1, 2])\nb2 = 0\nx = np.array([0.465, 0.178])\ny = 0","metadata":{},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Case A\n\nact_fun = 'relu'\nloss_fun = 'cross_entropy'\n\n# Add your test code. The result should look like below","metadata":{},"execution_count":25,"outputs":[{"name":"stdout","output_type":"stream","text":"\nu1 :\n [ 2.248 -0.426  2.425]\n\nz1 :\n [2.248 0.    2.425]\n\nu2 :\n 7.098\n\nz2 :\n 0.9991739261777141\n\nloss :\n 7.0988264152093095\n\ngradient g_A1:\n [[[0.46461588 0.17785296]\n  [0.         0.        ]\n  [0.92923175 0.35570592]]]\n\ngradient g_b1 :\n [[0.99917393 0.         1.99834785]]\n\ngradient g_A2 :\n [[2.24614299 0.         2.42299677]]\n\ngradient g_b2 :\n 0.9991739261777141\n"}]},{"cell_type":"code","source":"# Test Case B\n\nact_fun = 'tanh'\nloss_fun = 'mse'\n\n# Add your test code","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"7. (1 point) Put everything together: with the same initial parameter values and training instance as in Q6, train your model for 300 epochs with a learning rate of 0.01 under 4 cases:\n    1. $f$ = `relu` and $g$ = `cross_entropy`\n    1. $f$ = `relu` and $g$ = `mse`\n    1. $f$ = `tanh` and $g$ = `cross_entropy`\n    1. $f$ = `tanh` and $g$ = `mse`\n\n\nPlot the loss and $z^{(2)}$ obtained in each epoch as a line chart. An example is shown below.","metadata":{}},{"cell_type":"code","source":"A1 = np.array([[-2,1],[2, -2],[-1, 5]])\nA2 = np.array([1, -2, 2])\nb1 = np.array([3, -1, 2])\nb2 = 0\nx = np.array([0.465, 0.178])\ny = 0\n\nact_fun = 'relu'\nloss_fun = 'cross_entropy'\nepoch = 300\nlam = 0.01","metadata":{},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Add your code\n\n# Case A curve should look like below.\n\n# Also show curve for the other cases\n","metadata":{},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlAAAADgCAYAAADMt1gZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxCUlEQVR4nO3deXicZbnH8e89mexLmzTpmu50oYVCV1p2ARUQAUGRIi4IVjyCeuQcBffDcUGPetQDCCiKiIAIqIjsUGRtaaGldIWuNKV7m7TZt/v8MW9KWtK0SSbzziS/z3XNNe82M7++7Ty9512ex9wdERERETl8kbADiIiIiKQaFVAiIiIiHaQCSkRERKSDVECJiIiIdJAKKBEREZEOUgElIiIi0kEqoKTTzGy9mZ0Rdg4RkXho3aaZ2TfM7LedfJ9lZnZqPLNJ8omGHUBERCTZuPsPD2c7M7sDKHP3b7V67cTuyiXJQ0egRESkxzEzHSCQbqUCSrrMzDLN7Bdm9k7w+IWZZQbris3sYTMrN7NdZva8mUWCdV83s01mttfMVpnZ6eH+SUQk2QWn2a4zs+VmttvMfm9mWWZ2qpmVBe3KFuD3ZhYxs2vNbI2Z7TSz+8ysqNV7fdLMNgTrvnnA53zPzO5qNX+imb0UtGUbzewzZjYH+ATwNTOrNLN/tMrYciqwvfaxJfM1ZrbNzDab2WUJ2I0SByqgJB6+CcwEjgWOAWYALYezrwHKgBJgAPANwM1sHHAVMN3d84EPAusTmlpEUtUniLUZo4GxvNveDASKgOHAHOBq4HzgFGAwsBu4CcDMJgC/Bj4ZrOsHlLb1YWY2HHgU+D9ibdmxwGJ3vw34E/ATd89z9w+38fL22seWzH2AIcDlwE1mVnj4u0LCogJK4uETwPXuvs3dtwP/RaxRAmgABgHD3b3B3Z/32ACMTUAmMMHM0t19vbuvCSW9iKSaG919o7vvAn4AzA6WNwPfdfc6d68BrgS+6e5l7l4HfA/4aHB676PAw+7+XLDu28Hr23IJ8JS73xO0YzvdffFhZm2vfYRYG3l98L6PAJXAuMN8bwmRCiiJh8HAhlbzG4JlAP8DrAaeMLO1ZnYtgLuvBr5CrEHbZmb3mtlgREQObWOr6dbtzXZ3r221bjjw1+C0WzmwgtiPtwHBa/a9j7tXATsP8nlDgc7+wGuvfQTY6e6NreargbxOfpYkkAooiYd3iDVULYYFy3D3ve5+jbuPAs4FvtpyrZO73+3uJwavdeDHiY0tIilqaKvpfe0NsXaktY3AWe7et9Ujy903AZtbv4+Z5RA7jdeWjcROF7blwM880EHbR0ltKqAkHu4BvmVmJWZWDHwHuAvAzM4xsyPMzIAKYr/+ms1snJmdFlxMWQvUcPDD5yIirX3RzEqDC8K/Cfz5INvdAvwguIaJoI06L1h3P3BOcHF4BnA9B/8/8U/AGWZ2kZlFzayfmR0brNsKjGon60HbR0ltKqAkHr4PLASWAG8ArwXLAMYATxE7r/8ycLO7zyV2/dMNwA5gC9AfuC6xsUUkRd0NPAGsJXZq7fsH2e6XwEPELiHYC8wDjgNw92XAF4P32kzsAvOytt7E3d8GziZ2U8wuYDGxC8IBbid2LWe5mf2tjZe31z5KCrPY9bwiIiLJz8zWA1e4+1NhZ5HeTUegRERERDrokAWUmf0u6OBraatlRWb2pJm9FTyrzwoRSQpttVkHrP+EmS0xszeCjhGPaWs7EZH2HM4RqDuAMw9Ydi3wtLuPAZ4O5kVEksEdvLfNam0dcIq7Hw38N3BbIkJJfLj7CJ2+k2RwyALK3Z8jdtFca+cBfwim/0Csp1cRkdAdpM1qvf4ld98dzM7jIL1Pi4i0p7PXQA1w983B9BZinZKJiKSay4kN0SEi0iFdHq3a3d3MDnorXzDY4hyA3NzcqePHj+/qR4pIinj11Vd3uHtJ2DnaYmbvI1ZAndjONmq/RHqx9tqwzhZQW81skLtvNrNBwLaDbRgMtngbwLRp03zhwoWd/EgRSTVmtuHQWyWemU0Cfkusl+qDDd+h9kukl2uvDevsKbyHgE8H058G/t7J9xERSSgzGwY8CHzS3d8MO4+IpKZDHoEys3uAU4FiMysDvkusB+n7zOxyYgMjXtSdIUVEDtdB2qx0AHe/hdhQGv2Am2MjDNHo7tPCSSsiqeqQBZS7zz7IqtPjnEVEpMvaabNa1l8BXJGgOCLSQ6knchEREZEOUgElIiIi0kEqoEREREQ6SAWUiIiISAepgBIRERHpIBVQIiIiIh2kAkpERESkg1RAiYiIiHSQCigRERGRDlIBJSIiItJBKqBEREREOkgFlIiIiEgHqYASERER6SAVUCIiIiIdpAJKREREpINUQImIiIh0kAooERERkQ5SASUiIiLSQSqgRKRHMbPfmdk2M1t6kPVmZr8ys9VmtsTMpiQ6o4ikPhVQItLT3AGc2c76s4AxwWMO8OsEZBKRHkYFlIj0KO7+HLCrnU3OA+70mHlAXzMblJh0ItJTRMMOICKSYEOAja3my4Jlm+Px5o1NzXzit/PJzYySk5FGbkaU3MwouZlp9MlOZ2CfLAb1yWJAQRYDC7KIpul3rEgqUgElInIQZjaH2Gk+hg0bdlivaWhyHNi2t5bquiaq6hupCp7d9982Mxph/MB8JgwuYPLQQk4aW8ygPtlx/lOISHdQASUivc0mYGir+dJg2Xu4+23AbQDTpk3ztrY5UHZGGvd9flZb78Wemka27Kllc0UNmytqWbu9kmXv7OHRpVu455XYQbHxA/M5f/IQLpg8hP4FWR37k4lIwnSpgDKzfweuABx4A7jM3WvjEUxEpJs8BFxlZvcCxwEV7h6X03ftMTP65KTTJyedcQPz91vn7ry5tZLn3tzOY8u2cMOjK/mfx1dx3jGD+dLpYxhRnNvd8USkgzpdQJnZEOBLwAR3rzGz+4CLid0BIyISCjO7BzgVKDazMuC7QDqAu98CPAKcDawGqoHLwkn6LjNj3MB8xg3M53Mnj2LN9krunv82f5q/gb+//g4XTx/KtWeNJz8rPeyoIhLo6im8KJBtZg1ADvBO1yOJiHSeu88+xHoHvpigOJ0yuiSPb58zgc+fMoqb567hzpfX88zKbfzogqM5dVz/sOOJCF3oxsDdNwE/Bd4mdvdKhbs/ceB2ZjbHzBaa2cLt27d3PqmISC/TPz+L7507kQe+cDy5mVE+8/sF/PyJVTQ3H9blWCLSjTpdQJlZIbH+VEYCg4FcM7v0wO3c/TZ3n+bu00pKSjqfVESkl5o8rJCHrz6Ri6aV8qtnVvPFu1+jrrEp7FgivVpXOiA5A1jn7tvdvQF4EDg+PrFERKS1rPQ0fnzhJL71oSN5dOkWLr9jIbUNKqJEwtKVAuptYKaZ5ZiZAacDK+ITS0REDmRmXHHSKP7no5N4cc0Orrr7NRqbmsOOJdIrdeUaqPnA/cBrxLowiBD0lyIiIt3nY9OG8l/nTuSpFdv4xl/fwA/soVNEul2X7sJz9+8Su0VYREQS6FOzRrCjsp5fPf0WQwtzuPr0MWFHEulVNAiTiEiK+vczxnD+sYP5+VNv8sJbO8KOI9KrqIASEUlRZsYPLziaMf3z+PK9i9hSoYEgRBJFBZSISArLyYhy8yemUNPQxJfvXaQ+okQSRAWUiEiKO6J/Pt87dyLz1+3irvkbwo4j0iuogBIR6QE+NrWUk8eWcMOjK9m4qzrsOCI9ngooEZEewMy44YKjiZjx9QeWqGsDkW6mAkpEpIcY3Deb684ez0trdvLQ6xrbXaQ7qYASEelBZk8fxqTSPvzwkRVU1TWGHUekx0rKAuqZlVv52C0vaZwnEZEOikSM7354Ilv31HHzs6vDjiPSYyVlAdXcDAvW7+a1DbvDjiIiknKmDi/kgslD+M1z69iwsyrsOCI9UlIWUDNH9yMaMZ5frZ51RUQ64+tnjSeaZvzP46vCjiLSIyVlAZWXGWXysL4amkBEpJMGFGTx2RNG8vCSzSx7pyLsOCI9TlIWUAAnHlHC0ncq2F1VH3YUEZGU9LmTR9EnO52f6iiUSNwlbwE1phh3eHGNjkKJSMeY2ZlmtsrMVpvZtW2sH2Zmc81skZktMbOzw8jZ3fpkp3PlKaOZu2o7C9bvCjuOSI+StAXUMaV9yM+K6jSeiHSImaUBNwFnAROA2WY24YDNvgXc5+6TgYuBmxObMnE+c/wI+udn8pPHVqpzTZE4StoCKpoWYdaofjz/1g596UWkI2YAq919rbvXA/cC5x2wjQMFwXQfoMf2OpmdkcbVp49hwfrdPLtqe9hxRHqMpC2gAE4aU8ym8hrW79S4TiJy2IYAG1vNlwXLWvsecKmZlQGPAFe39UZmNsfMFprZwu3bU7f4uHj6UEoLs/nl02/pB6lInCR1AXXimBIAXngrdRsuEUlKs4E73L0UOBv4o5m9pz1099vcfZq7TyspKUl4yHhJT4vwhVNHs3hjOS+u3hl2HJEeIakLqBH9chjSN5vndR2UiBy+TcDQVvOlwbLWLgfuA3D3l4EsoDgh6ULy0amlDCjI5Ma5b4UdRaRHSOoCysw4aUwxL6/ZSWNTc9hxRCQ1LADGmNlIM8sgdpH4Qwds8zZwOoCZHUmsgOrRh7ozo2nMOXk089bu0h15InGQ1AUUwEljSthb18jijeVhRxGRFODujcBVwOPACmJ32y0zs+vN7Nxgs2uAz5nZ68A9wGe8F1wcNHvGUPrlZnDjMxojT6SromEHOJQTxxSTFjHmrtrGtBFFYccRkRTg7o8Quzi89bLvtJpeDpyQ6Fxhy8mIcvlJI/nJY6tYUlbOpNK+YUcSSVlJfwSqT3Y6U4cX8szKHn10XUQkIT45czgFWVFumqujUCJd0aUCysz6mtn9ZrbSzFaY2ax4BWvttPH9WbF5D5srarrj7UVEeo38rHQ+c8JIHl+2lVVb9oYdRyRldfUI1C+Bx9x9PHAMsesN4u608f0BmKujUCIiXfbZE0aQm5HGr5/VUSiRzup0AWVmfYCTgdsB3L3e3cvjlGs/Y/rnMaRvNnNXbeuOtxcR6VX65mRwyXHD+MeSzbytjopFOqUrR6BGErvt9/fBgJy/NbPcOOXaj5nxvvElvLh6B3WNTd3xESIivcoVJ40izYxbn1sTdhSRlNSVAioKTAF+HQzIWQW0Nep5XIZCOG18f6rrm5i/Vv2XiIh01YCCLC6cWspfXi1j257asOOIpJyuFFBlQJm7zw/m7ydWUO0nXkMhzBpVTGY0wjMrdRpPRCQerjxlFI1Nzdz+wrqwo4iknE4XUO6+BdhoZuOCRacDy+OSqg3ZGWnMGt2Puau2aTBMEZE4GN4vl3MmDeaueRuoqG4IO45ISunqXXhXA38ysyXAscAPu5yoHaeN78+GndWs3VHVnR8jItJrfOHU0VTVN3Hny+vDjiKSUrpUQLn74uD03CR3P9/dd8crWFtaujN4avnW7vwYEZFe48hBBZw2vj+/e3Ed1fWNYccRSRlJ3xN5a6WFOUwcXMDjy7aEHUVEpMf44vtGs7u6gXtf2Rh2FJGUkVIFFMAHJw7ktbfLddeIiEicTB1exIyRRfzm+bXUNzaHHUckJaRkAQXwhE7jiYjEzb+dOprNFbX8bdGmsKOIpISUK6DGDshjRL8cncYTEYmjU8aWMHFwAbf8aw1NzbrTWeRQUq6AMjM+OHEgL6/ZSUWNbrsVEYkHM+PfTj2CtTuqeGypfqCKHErKFVAAH5g4gMZm51mNjSciEjdnHjWQUcW53PzsavW3J3IIKVlATR5aSEl+pk7jiYjEUVrEuPKU0Sx7Zw/PvbUj7DgiSS0lC6hIxHj/hAE8u2o7tQ0aXFhEJF7OnzyEQX2yuGnu6rCjiCS1lCygIHY3XnV9E8/rV5KIHMDMzjSzVWa22szeM8h5sM1FZrbczJaZ2d2JzpisMqIRrjhpFK+s28WrGzR4u8jBpGwBNWtUP/pkp/PPJe+EHUVEkoiZpQE3AWcBE4DZZjbhgG3GANcBJ7j7ROAric6ZzGbPGEphTjo3z10TdhSRpJWyBVRGNMJZRw3kyeVbdRpPRFqbAax297XuXg/cC5x3wDafA25qGX7K3XVHSis5GVEuO2EkT6/cxorNe8KOI5KUUraAAjhn0mCq6puYu1Jtn4jsMwRoPSZJWbCstbHAWDN70czmmdmZbb2Rmc0xs4VmtnD79u3dFDc5fXrWCHIz0vj1szoKJdKWlC6gZo4qojgvg4eXbA47ioikligwBjgVmA38xsz6HriRu98WDJg+raSkJLEJQ9YnJ51LZw7n4SXvsH5HVdhxRJJOShdQ0bQIZx01iKdXbqWyTqOIiwgAm4ChreZLg2WtlQEPuXuDu68D3iRWUEkrl584kvS0CP/3jO7IEzlQShdQAB8+ZjC1Dc08vUJj44kIAAuAMWY20swygIuBhw7Y5m/Ejj5hZsXETumtTWDGlNC/IItPzRrOXxeVsXrb3rDjiCSVlC+gpg0vZGBBFv94XafxRATcvRG4CngcWAHc5+7LzOx6Mzs32OxxYKeZLQfmAv/p7jvDSZzcrjxlNNnpafzvk2+FHUUkqaR8ARWJGB+aNIh/vblNY+OJCADu/oi7j3X30e7+g2DZd9z9oWDa3f2r7j7B3Y9293vDTZy8+uVl8tkTR/LPNzazdFNF2HFEkkbKF1AQO43X0OQa2kVEpBtccdIoCrKi/O+Tb4YdRSRp9IgC6pjSPozol8ODr5WFHUVEpMfpk53O508ZzdMrt/Hqht1hxxFJCj2igDIzLphSyry1u9i4qzrsOCIiPc5lJ4ygOC+THz2yAncPO45I6HpEAQXwkcmxfvL+uujAu5VFRKSrcjKiXPOBsSzcsJvHlupyCZEeU0ANLcph5qgiHnytTL+ORES6wUXThjJuQD43PLaS+sbmsOOIhKrHFFAAF04pZf3Oap2jFxHpBmkR4xsfOpINO6u58+X1YccRCVWPKqDOOnoQ2elpPKCLyUVEusUpY0s4eWwJ//fMasqr68OOIxKaLhdQZpZmZovM7OF4BOqKvMwoZx01kIdf30xtQ1PYcUREeqRvnn0ke2sb1K2B9GrxOAL1ZWK9/SaFC6eWsreukSeWa2gXEZHuMG5gPpfOHM4f521Q55rSa3WpgDKzUuBDwG/jE6frZo3qx5C+2fx5wdthRxER6bGu+cA4inIz+NbfltLcrBt3pPfp6hGoXwBfA5LmdoxIxJg9Yygvrt7Juh1VYccREemR+mSn842zj2TxxnL+vHBj2HFEEq7TBZSZnQNsc/dXD7HdHDNbaGYLt2/f3tmP65CLpg0lGjHueUVHoUREustHJg9hxsgifvzYSnZV6YJy6V26cgTqBOBcM1sP3AucZmZ3HbiRu9/m7tPcfVpJSUkXPu7w9S/I4owjB3D/q2XUNepichGR7mBm/Pd5R1FZ28gP/pk0l8KKJESnCyh3v87dS919BHAx8Iy7Xxq3ZF10yXHD2FVVrx5zRUS60biB+Xz+lFE88FoZz6zUzTvSe/SofqBaO/GIYoYV5XD3fJ3GExHpTl86fQzjBuRz3YNvUFHdEHYckYSISwHl7s+6+znxeK94iV1MPoz563axetvesOOIiPRYmdE0fvqxY9hRWc/1Dy8PO45IQvTYI1AAH5tWSnqacdc8HYUSEelOR5f24d9OHc0Dr5Xx9AqdypOer0cXUMV5mXzo6EHc/2oZe2t1WFlEpDtdfdoYxg/M5+sPLGHb3tqw44h0qx5dQAF89sSRVNY18ucF6qdEpLcwszPNbJWZrTaza9vZ7kIzczOblsh8PVVGNMKvZk+msq6Rr/75dXWwKT1ajy+gJpX2ZfqIQu54aT1N+jKL9HhmlgbcBJwFTABmm9mENrbLJzYU1fzEJuzZxg7I53sfnsgLq3fw63+tCTuOSLfp8QUUwOUnjqRsdw1PLleXBiK9wAxgtbuvdfd6Yv3UndfGdv8N/BjQuaY4+/j0oXz4mMH8/Mk3Wbh+V9hxRLpFryig3j9hIKWF2dz+wrqwo4hI9xsCtD5nXxYs28fMpgBD3f2f7b1RGCMp9ARmxg8/chRD+mZz1d2LdD2U9Ei9ooBKixifOX4EC9bvZklZedhxRCREZhYBfg5cc6htwxhJoafIz0rnlkunUlHTwJV/fFWjQkiP0ysKKIgdUs7LjPLb53UUSqSH2wQMbTVfGixrkQ8cBTwbDEU1E3hIF5LH34TBBfzsomN47e1yvv23pbjrOlTpOXpNAZWflc4lxw3j4SXvsH5HVdhxRKT7LADGmNlIM8sgNtTUQy0r3b3C3YvdfUQwFNU84Fx3XxhO3J7t7KMH8aXTjuC+hWXc8dL6sOOIxE2vKaAArjhpJNG0CL9+VneGiPRU7t4IXAU8DqwA7nP3ZWZ2vZmdG2663ukrZ4zlAxMG8N8PL+fxZbqZR3qGXlVA9c/PYvb0oTzwWhmbymvCjiMi3cTdH3H3se4+2t1/ECz7jrs/1Ma2p+roU/eKRIxfXjyZY4b25ep7FjF/7c6wI4l0Wa8qoADmnDIaM7hV/ZOIiCRMdkYav/v0dIYWZnPFnQtZuWVP2JFEuqTXFVBD+mZz4ZRS7l2wkW17dGutiEiiFOZmcOflx5GTkcanbn+Ftdsrw44k0mm9roAC+MKpo2lsauY3z68NO4qISK8ypG82f7z8OJqandm/maciSlJWryyghvfL5fzJQ7jz5Q1sqdBRKBGRRBo7IJ+7PzeTxiYVUZK6emUBBfDvZ4yl2Z1fPv1m2FFERHqdcQPfLaIuvm2eromSlNNrC6ihRTl84rjh/HnBRlZv068fEZFEGzcwn3vmzMQMPnbLy7yyTuPmSerotQUUwFWnHUF2eho/fXxV2FFERHqlsQPyeeALx1OSn8mlt89XP1GSMnp1AVWcl8mck0fz2LItvPb27rDjiIj0SqWFOdx/5fFMGFTAlXe9ym3PrdGwL5L0enUBBbHeyYvzMrjhkZX6woqIhKQoN4O7P3ccZx01kB8+spKv3vc6tQ0agFiSV68voHIzo3zljLG8sn4XDy/ZHHYcEZFeKycjyk2XTOGr7x/LXxdt4uO3vkzZ7uqwY4m0qdcXUACzZwxj4uACfvjICqrqGsOOIyLSa5kZXzp9DLd+ciprtldx9i+f57Gl+nEryUcFFJAWMa4/byKbK2q5ae7qsOOIiPR6H5w4kH9+6URGFOdy5V2v8a2/vaFTepJUVEAFpg4v4oIpQ/jt8+tYo07dRERCN7xfLvdfeTxzTh7FXfPe5rwbX2RJWXnYsUSALhRQZjbUzOaa2XIzW2ZmX45nsDBcd9aRZGekce0DS2hu1gXlIiJhy4hG+MbZR3LHZdPZXV3P+Te9yI8eXaGjURK6rhyBagSucfcJwEzgi2Y2IT6xwlGSn8m3z5nAgvW7+dP8DWHHERGRwKnj+vPkV0/h49OHcuu/1nLWL59n3tqdYceSXqzTBZS7b3b314LpvcAKYEi8goXlwilDOGlMMTc8upJN5TVhxxERkUCf7HR+dMEk7r4iNhjxxbfN4+p7FqmtllDE5RooMxsBTAbmt7FujpktNLOF27dvj8fHdSsz44cfORoH/uO+12nSqTwRkaRy/BHFPP6Vk/ny6WN4YtkWTv/Zs/ziqTepqddpPUmcLhdQZpYHPAB8xd3fMxqku9/m7tPcfVpJSUlXPy4hhhbl8N0PT+DltTu57bm1YccREZEDZGek8e/vH8vT15zCGUcO4BdPvcWpP53LH+dtoL6xOex40gt0qYAys3RixdOf3P3B+ERKDhdNG8rZRw/kZ0+s0l0fIinGzM40s1VmttrMrm1j/VeDG2CWmNnTZjY8jJzSdaWFOdx4yRTu+/wshhbm8O2/LeW0nz3LfQs30tikQkq6T1fuwjPgdmCFu/88fpGSg5nxo49MoiQ/k6vvWURFTUPYkUTkMJhZGnATcBYwAZjdxg0ui4Bp7j4JuB/4SWJTSrzNGFnEX66cxR2XTacwJ4Ov3b+E9/3sWe58eb1O7Um36MoRqBOATwKnmdni4HF2nHIlhT456dx4yWTeKa/hK/cuUtcGIqlhBrDa3de6ez1wL3Be6w3cfa67t4wRMg8oTXBG6QZmxqnj+vPQVSdw6yenUpyXyXf+vozjb3ia/33yTXZW1oUdUXqQrtyF94K7m7tPcvdjg8cj8QyXDKYOL+I750xg7qrt/OLpt8KOIyKHNgTY2Gq+jPbvEL4ceLRbE0lCmRkfnDiQB79wPH+5chZThxfyy6ff4vgbnuE//vI6i97ercHjpcuiYQdIBZfOHM7rZRX86um3GF2Sy3nHpnxvDSICmNmlwDTglIOsnwPMARg2bFgCk0k8mBnTRxQxfUQRq7ft5fYX1vP3xZu4/9UyJgwq4JLjhnH+5CHkZeq/Quk4DeVyGMyM759/FDNGFvGff1nCy2vUeZtIEtsEDG01Xxos24+ZnQF8EzjX3ds8t5OKdxFL247on8+PLjia+d84ne+ffxQOfOtvS5n+/af40j2LeGblVhp00bl0gCXyMOa0adN84cKFCfu8eKuobuDCW15i655a7vv8LI4cVBB2JJGkZmavuvu0BH9mFHgTOJ1Y4bQAuMTdl7XaZjKxi8fPdPfDOjef6u2X7M/dWbSxnAdeLeOfb2ymvLqBotwMPnT0IM6fPJjJQwuJRCzsmBKy9towFVAdtKm8hgtvfon6pmbu+dxMxg3MDzuSSNIKo4AKPvds4BdAGvA7d/+BmV0PLHT3h8zsKeBoYHPwkrfd/dz23rMntF/StvrGZp57czt/W7yJJ5dvpa6xmf75mbx/wgA+MHEgs0b1IyOqEza9kQqoOFu3o4qP3/oyTc3OPXNmMnaAiiiRtoRVQHWHntJ+Sfsq6xp5cvkWnly+lWdXbae6vom8zCinjivh9CP7c8IRxfTPzwo7piSICqhusHZ7JRffNo+GpmZu/8x0pgwrDDuSSNJRASWprLahiZfW7ODJ5Vt5cvk2dgTdIIwfmM/JY0s48YhiZowsIis9LeSk0l1UQHWTDTur+NTvXmHrnlpunD2FMyYMCDuSSFJRASU9RXOzs3zzHp57azsvvLWDhet3U9/UTEY0wtRhhUwfWcT0EYVMGVZIru7q6zFUQHWjHZV1fPaOBSzdVMHXzhzP508eRayTdhFRASU9VXV9I/PX7eL5N3cwb+1OVmzZgzukRYwJgwqYNqKQ6SOKmDq8kAEFOuWXqtprw1Qmd1FxXib3zpnJf96/hBseXcnrG8v5yUcnkZ+VHnY0ERHpJjkZUd43rj/vG9cfgD21DSx6u5yF63exYP0u7p7/Nr9/cT0AJfmZTBrSh6NL+3B08KzrqFKfCqg4yMmIcuPsyRxb2pcfPbqCJWUV/PRjxzBrdL+wo4mISAIUZKVzytgSThkb6y+svrGZpe9UsPjtcpZuqmDJpgqeWbWNlpM+AwoyOXpIXyYMymfswHzGDchnRHEu6Wm62y9VqICKEzPjcyePYsrwQq65bzGzfzOPy04Ywdc+OJ7sDF1gKCLSm2REI0wZVrjfDUaVdY0sf2cPS8paFVUrt9IyzGpGWoRRJbmMG5jP2AGxomrMgDyG9M0mqsIq6aiAirOpwwt55MsnccOjK/n9i+t5fOkWrjv7SM6ZNEjXRomI9GJ5mVFmjCxixsiifctqG5pYs72SN7fuZdWW2PPC9bv5++J39m2TnmYMLcphZL9cRhTnMjJ4jCjOZVBBljr8DIkKqG6QkxHl+vOO4kNHD+K//rGcq+9ZxJ0vr+cbZx/JZHV3ICIigaz0NCYO7sPEwX32W763toG3tlWyemsl63ZWsW57Fet3VvHimh3UNrw75ExmNMLwfjkMLcyhtDCbIYXZlAbTpYU5FOak68d7N1EB1Y2OG9WPf1x9Ivct3MhPH1/FR25+iZPGFHP1aWP2+wUiIiLSWn5W+ntOAUKsO4Ute2pZv6OKdTurYs87qinbXc0r63axt65xv+2z09P2FVZD+mYzqE8WAwpij4F9shiQn0VBdlRFVieogOpmaRFj9oxhnHvMYO6at4HfPL+Wi259mcnD+vKpWcM566hB6oRNREQOSyRiDO6bzeC+2Rx/RPF71lfUNFC2u5pNu2soCx6byqsp213DorfLqahpeM9rstIjDCzIon9BFgMLshhQkMmAYL44N4N+eZn0y8ugMCeDNJ0u3Ef9QCVYTX0T9y54mztf3sC6HVUU5WZwweQhnHvsYI4e0ke/AqRHUT9QIsmltqGJrXtq2bqnji17atlaUcvWPbVs2VPLtmDZlj211Dc2v+e1EYOi3Az65cYKqn55mfTLzaA4L4PivEz65WVSlJtB35x0+man0yc7PeUvflc/UEkkOyONy04YyadnjeClNTv547z1/OHl9fz2hXUM75fDhycN5oMTBzJxcIEuDBQRkbjKSk9jeL9chvfLPeg27k5FTQPb9taxs7KenVV17Nhbx86qenZU1rOzMjb9Rlk5Oyvr33PasLX8rGhQUAWFVU4GfbPT6ZsTK7AKc1qWp9MnO4OCrCj5WelkpUeS/oCCCqiQRCLGiWOKOXFMMRXVDTy2bDP/eH0zNz+7mhvnrqY4L5OTxxZz6rj+zBxVpE7XREQkIcwsVujkZMBhjFBW29DErqp6dlbWs6Oqjj01Deyuqqe8poHy6gYqahoor65nd3UDZbtrKK+up6KmYV/3DW2JRoz8oJjKy4zumy7IipKX9e78vudW2+RlRcnLiJKbmdatR8BUQCWBPjnpfHz6MD4+fRg7Kuv416rt/OvN7TyzchsPvrYJgGFFOUwbUci04UVMGd6X0SV56nBNRERCl5Wetu+6rMPV3OzsrW2kvKae8uqGoNiqZ09tI5W1jeytbWBv8FxZ18ie2kY2ldewMpjfW9tIU3sVWCAzGiE3M1ZM5WZEycuM8ofPzojLeIUqoJJMcV4mF04t5cKppTQ1O29sqmDBul0s3LCL597cvq+gykiLMGZAHkcOKgge+RxRkkdJfmbSH/YUEZHeLRIx+uSk0ycnneGdGLTD3alpaNpXZMWeY4/KugYq65qoqmukqr4x9lzXRGVdI9X1jWRE43PwQQVUEkuLGMcO7cuxQ/vyOUbh7qzfWc3ijbtZsXkvKzbv4dlV27j/1bJ9r8nNiJ3fHlGcw4ig07WhhTkM6hO7ZVV3/ImISKozM3IyouRkREMbrFkFVAoxs3090H5k8rvLt++tY8XmPazbUcW6HVVs2FnFis17eWLZVhoPOMRZlJvBwIIsBveNFVT987MoCu6iaLmDojg3U/2CiIiItEMFVA9Qkp9JSX4JJweDWLZoaGpm0+4a3imv4Z2KWrZUtDzXUra7hoUbdlNe/d4+QSB2AV9RbgZFuRkUZMcu3CvISt83nZ+VTkH2u8vys2LnlnMzo2RnpJGT3r0X74mIiISpSwWUmZ0J/BJIA37r7jfEJZXERXpahBHBeEkHU9/YzO7qenZU1u27i2JnVXCbamU9u6rr2VPTwDvltays3cuemgb21jVyON2HZUQj5ATFVHZGWqy4Sk+LLcuM7lueGY2QGQ2e0yNkpEXITN9/eUY0EqxvWd6yLI2MaIT0NCMaiT3ryJkcqm0ys0zgTmAqsBP4uLuvT3ROEUldnS6gzCwNuAl4P1AGLDCzh9x9ebzCSffLiEb2det/uJqbncr62MV6e2oaYo/gwr3q+iaq65pizw2N1NQ3UVXXRE1DY2xZfRPbK+uo3lVNTTBf39hMXWNTu7e0dkRaxIhGjIy0CNE0I5oWIT0Se46mGemRCOnRdwuuaCRYnhYhGjHSo+9un55mRMxIi8Seo5FgOmKkBctbHrHtIC0SIc3Yt1008u57tH6fg79Hq4cZZhAxIxIJni12OrdlOhIUjJHIu/P7XrPf9u8ue3f9u9v3lMLzMNumy4Hd7n6EmV0M/Bj4eOLTikiq6soRqBnAandfC2Bm9wLnASqgerhIxGKn7rLSGdKB21bb4+40NntQTMUKqrqGZuqbmqlrCOYbm/cVW3WN+y9vbHYam5ppaHIam5tpbHLqm2LPjc3B8qZmGlpt19BqfU1DG68Ltm9udprcaQqmG5ud5pb5xHXk3+1aF1XtFVynjR/Azy46Juy47Tmctuk84HvB9P3AjWZmnsihGUQkpXWlgBoCbGw1XwYc17U40luZGenBUaDczLDTHD4PCqkmd5qbiRVaTa0Krpb1za23ixVhTfsVYk5j0wHv09xMczM0e6xQ8+A5Nu+4v7suNt+y3f7z+7++9faxo4kHvr7d7d0ZP7Ag7N1+KIfTNu3bxt0bzawC6AfsSEhCEUl53X4RuZnNAeYEs5VmtuowX1pM8jZmyZotWXOBsnVGsubiE4efbXh3Z+lOXWi/IHn//pI1FyRvtmTNBcrWGR3JddA2rCsF1CZgaKv50mDZftz9NuC2jr65mS1M1kFIkzVbsuYCZeuMZM0FyZ2Nw2ubWrYpM7Mo0IfYxeT76Wz7Bcm7j5I1FyRvtmTNBcrWGfHK1ZX7zBcAY8xspJllABcDD3U1kIhIFx1O2/QQ8Olg+qPAM7r+SUQ6otNHoILrBq4CHid2q/Dv3H1Z3JKJiHTCwdomM7seWOjuDwG3A380s9XALmJFlojIYevSNVDu/gjwSJyyHKhTh80TJFmzJWsuULbOSNZckNzZ2myb3P07raZrgY91c4xk3UfJmguSN1uy5gJl64y45DIdtRYRERHpGI21ISIiItJBSVlAmdmZZrbKzFab2bUhZ1lvZm+Y2WIzWxgsKzKzJ83sreC5MEFZfmdm28xsaatlbWaxmF8F+3CJmU0JIdv3zGxTsO8Wm9nZrdZdF2RbZWYf7MZcQ81srpktN7NlZvblYHmo+62dXMmwz7LM7BUzez3I9l/B8pFmNj/I8OfgAm3MLDOYXx2sH9Fd2VJBMrVfQR61YZ3LlQzfxaRsvw6RLdT9ltD2y4MO9JLlQeyizzXAKCADeB2YEGKe9UDxAct+AlwbTF8L/DhBWU4GpgBLD5UFOBt4FDBgJjA/hGzfA/6jjW0nBH+vmcDI4O87rZtyDQKmBNP5wJvB54e639rJlQz7zIC8YDodmB/si/uAi4PltwBfCKb/DbglmL4Y+HN3/ltL5keytV9BJrVhncuVDN/FpGy/DpEt1P2WyPYrGY9A7RuGwd3rgZZhGJLJecAfguk/AOcn4kPd/TlidwwdTpbzgDs9Zh7Q18wGJTjbwZwH3Ovude6+DlhN7O+9O3JtdvfXgum9wApivVCHut/ayXUwidxn7u6VwWx68HDgNGLDnsB791nLvrwfON2shwys13Gp0H6B2rDDyXUwvb79OkS2g0nIfktk+5WMBVRbwzC095fS3Rx4wsxetVivxAAD3H1zML0FGBBOtHazJMt+vCo4lPy7VqcJQskWHJqdTOwXSdLstwNyQRLsMzNLM7PFwDbgSWK/FsvdvbGNz99vWBSgZViU3ihZvnetqQ3rvNC/iy2Stf1qIxuEvN8S1X4lYwGVbE509ynAWcAXzezk1is9dtwvKW5lTKYsgV8Do4Fjgc3Az8IKYmZ5wAPAV9x9T+t1Ye63NnIlxT5z9yZ3P5ZYL94zgPFh5JC4UBvWOUnxXYTkbb8gOduwRLVfyVhAHdYQMYni7puC523AX4n9ZWxtOSwaPG8LK187WULfj+6+NfiH3Az8hncP1yY0m5mlE/uC/8ndHwwWh77f2sqVLPushbuXA3OBWcROB7T0Hdf68/dls3aGReklQv/eHUhtWOcky3cxWduvg2VLlv0WZCmnG9uvZCygkmaIGDPLNbP8lmngA8BS9h8G4tPA38PIFzhYloeATwV3ZcwEKlod8k2IA869f4TYvmvJdnFw98NIYAzwSjdlMGK9Tq9w95+3WhXqfjtYriTZZyVm1jeYzgbeT+z6hrnEhj2B9+4zDYsSkzTtF6gN64ok+S4mZfvVXraw91tC268DrypPhgexOwneJHbe8psh5hhF7K6B14FlLVmInR99GngLeAooSlCee4gdEm0gdg738oNlIXYnwk3BPnwDmBZCtj8Gn70k+Ec6qNX23wyyrQLO6sZcJxI7vL0EWBw8zg57v7WTKxn22SRgUZBhKfCdVt+HV4hd/PkXIDNYnhXMrw7Wj0rE9yFZH8nSfrX6O1Mb1rlcyfBdTMr26xDZQt1viWy/1BO5iIiISAcl4yk8ERERkaSmAkpERESkg1RAiYiIiHSQCigRERGRDlIBJSIiItJBKqAkKZjZqWb2cNg5REQ6Su1X76QCSkRERKSDVEBJh5jZpWb2ipktNrNbg0EbK83sf81smZk9bWYlwbbHmtm8YFDJv7YMKmlmR5jZU2b2upm9Zmajg7fPM7P7zWylmf0p6OlWRCQu1H5JPKmAksNmZkcCHwdO8NhAjU3AJ4BcYKG7TwT+BXw3eMmdwNfdfRKxnmlblv8JuMndjwGOJ9YDMMRG8/4KMIFYr7EndPMfSUR6CbVfEm/RQ28iss/pwFRgQfDjKpvYIJbNwJ+Dbe4CHjSzPkBfd/9XsPwPwF+CcbmGuPtfAdy9FiB4v1fcvSyYXwyMAF7o9j+ViPQGar8krlRASUcY8Ad3v26/hWbfPmC7zo4PVNdqugn9+xSR+FH7JXGlU3jSEU8DHzWz/gBmVmRmw4n9O2oZ5foS4AV3rwB2m9lJwfJPAv9y971AmZmdH7xHppnlJPIPISK9ktoviStVyHLY3H25mX0LeMLMIsRGLv8iUAXMCNZtI3adAcCngVuCBmYtcFmw/JPArWZ2ffAeH0vgH0NEeiG1XxJv5t7Zo5UiMWZW6e55YecQEekotV/SWTqFJyIiItJBOgIlIiIi0kE6AiUiIiLSQSqgRERERDpIBZSIiIhIB6mAEhEREekgFVAiIiIiHaQCSkRERKSD/h/NwevXAcx6CAAAAABJRU5ErkJggg==\n","text/plain":"<Figure size 720x216 with 2 Axes>"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"8. (1 point) Analysis: \n    - Comparing Case A and B (i.e., different loss functions), do you notice any significant difference in the loss and prediction curves? Can you explain this difference `analytically`? Hint: think about gradients under these two cases.\n    - Comparing Case A and C (i.e., different activation functions), do you notice any significant difference in the loss and prediction curves? Can you explain this difference analytically? \n    - With your analysis, which functions would work the best for $f$ and $g$?\n    \n\nWrite your analysis as markdowns in the Juypter Notebook. Please clearly demonstrate your analysis using formulas or experiment results.\n","metadata":{}},{"cell_type":"markdown","source":"9.(Bonus) So far we trained the model with a single sample. If you train the model using a batch of training samples, do you think the observations you had before still hold? \n- Rewrite the train function so that the model can be trained by a batch of samples.\n- Train the model with the samples provided (`hw1_data.txt`; x: the first two columns, and y: the last column).\n- You may need to adjust `epoch` and `learning rate`to prevent underfitting or overfitting\n- Use plots to demonstrate your finding.","metadata":{}},{"cell_type":"code","source":"# Load training samples\n\ndata = np.loadtxt(\"hw1_data.txt\")\ndata[0:2]","metadata":{},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"array([[0.93629975, 0.61317151, 1.        ],\n       [0.46518579, 0.17846441, 0.        ]])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}